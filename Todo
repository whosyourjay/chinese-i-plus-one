- Script takes split as parameter
- Define word in context
- Give Youtube url and do full pipeline
  - Run yt-dlp and save reasonably short filename
  - Split by SRT
  - Segment each sub and package in csv
  - Run sentence picker against unknown words
  - Define each word in context
- Better frequency list. Why are we missing common characters like 一, 三, 多?
- Use word dataset to get definition, pinyin, and audio for the i+1 word
  - generate if missing
- Import list and venvs to enable more segmenters
- Reduce the n^2 dependence on i+1 bucket. Maybe already done?

- Cleanup test_segmenters
  - greedy_segmenter to match other formats
  - catch errors and warnings better
  - show test sentence output

Not sure these are good
- Add bold tags back at the end?
